<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DLT-META</title><link>https://databrickslabs.github.io/dlt-meta/</link><description>Recent content on DLT-META</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 04 Aug 2021 14:50:11 -0400</lastBuildDate><atom:link href="https://databrickslabs.github.io/dlt-meta/index.xml" rel="self" type="application/rss+xml"/><item><title>General</title><link>https://databrickslabs.github.io/dlt-meta/faq/general/</link><pubDate>Wed, 04 Aug 2021 14:50:11 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/faq/general/</guid><description>Q. What is DLT-META ?
DLT-META is a solution/framework using Databricks Delta Live Tables aka DLT which helps you automate bronze and silver layer pipelines using CI/CD.
Q. What are the benefits of using DLT-META ?
With DLT-META customers needs to only maintain metadata like onboarding.json, data quality rules and silver transformations and framework will take care of execution. In case of any input/output or data quality rules or silver transformation logic changes there will be only metadata changes using onboarding interface and no need to re-deploy pipelines.</description></item><item><title>Execution</title><link>https://databrickslabs.github.io/dlt-meta/faq/execution/</link><pubDate>Wed, 04 Aug 2021 14:26:55 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/faq/execution/</guid><description>Q. How do I get started ?
Please refer to the Getting Started guide
Q. How do I create metadata DLT-META ?
DLT-META needs following metadata files:
Onboarding File captures input/output metadata Data Quality Rules File captures data quality rules Silver transformation File captures processing logic as sql Q. What is DataflowSpecs?
DLT-META translates input metadata into Delta table as DataflowSpecs
Q. How many DLT pipelines will be launched using DLT-META?</description></item><item><title>Metadata Preparation</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/metadatapreperation/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/metadatapreperation/</guid><description>Create onboarding.json metadata file and save to s3/adls/dbfs e.g.onboarding file Create silver_transformations.json and save to s3/adls/dbfs e.g Silver transformation file Create data quality rules json and store to s3/adls/dbfs e.g Data Quality Rules Onboarding File structure env is your environment placeholder e.g dev, prod, stag
Field Description data_flow_id This is unique identifer for pipeline data_flow_group This is group identifer for launching multiple pipelines under single DLT source_format Source format e.</description></item><item><title>Build Python Wheel</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/buildwhl/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/buildwhl/</guid><description>This step is optional if you use PyPI dlt-meta
Clone DLT-META repo locally Launch terminal Goto root dlt-meta folder use command: python setup.py bdist_wheel Upload created python wheel file from dist folder to your blob storage like s3/adls/dbfs</description></item><item><title>Running Onboarding</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/runoboardingopt1/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/runoboardingopt1/</guid><description>Option#1: Python whl job Go to your Databricks landing page and do one of the following:
In the sidebar, click Jobs Icon Workflows and click Create Job Button.
In the sidebar, click New Icon New and select Job from the menu.
In the task dialog box that appears on the Tasks tab, replace Add a name for your jobâ€¦ with your job name, for example, Python wheel example.</description></item><item><title>Running Onboarding</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/runoboardingopt2/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/runoboardingopt2/</guid><description>Option#2: Notebook Copy below code to databricks notebook cells %pip install dlt-meta onboarding_params_map = { &amp;#34;database&amp;#34;: &amp;#34;dlt_demo&amp;#34;, &amp;#34;onboarding_file_path&amp;#34;: &amp;#34;dbfs:/onboarding_files/users_onboarding.json&amp;#34;, &amp;#34;bronze_dataflowspec_table&amp;#34;: &amp;#34;bronze_dataflowspec_table&amp;#34;, &amp;#34;bronze_dataflowspec_path&amp;#34;: &amp;#34;dbfs:/onboarding_tables_cdc/bronze&amp;#34;, &amp;#34;silver_dataflowspec_table&amp;#34;: &amp;#34;silver_dataflowspec_table&amp;#34;, &amp;#34;silver_dataflowspec_path&amp;#34;: &amp;#34;dbfs:/onboarding_tables_cdc/silver&amp;#34;, &amp;#34;overwrite&amp;#34;: &amp;#34;True&amp;#34;, &amp;#34;onboard_layer&amp;#34;: &amp;#34;bronze_silver&amp;#34;, &amp;#34;env&amp;#34;: &amp;#34;dev&amp;#34;, &amp;#34;version&amp;#34;: &amp;#34;v1&amp;#34;, &amp;#34;import_author&amp;#34;: &amp;#34;Ravi&amp;#34; } from src.onboard_dataflowspec import OnboardDataflowspec OnboardDataflowspec(spark, onboarding_params_map).onboard_dataflow_specs() Specify your onboarding config params in above onboarding_params_map
Run notebook cells</description></item><item><title>Launch Generic DLT pipeline</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/dltpipeline/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/dltpipeline/</guid><description>1. Create a Delta Live Tables launch notebook Go to your Databricks landing page and select Create a notebook, or click New Icon New in the sidebar and select Notebook. The Create Notebook dialog appears.
In the Create Notebook dialogue, give your notebook a name e.g dlt_meta_pipeline and select Python from the Default Language dropdown menu. You can leave Cluster set to the default value. The Delta Live Tables runtime creates a cluster before it runs your pipeline.</description></item><item><title>Additionals</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/additionals/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/additionals/</guid><description>This is easist way to launch dlt-meta to your databricks workspace with following steps.
Run Integration Tests Launch Terminal/Command promt
Goto to DLT-META directory
Create environment variables.
export DATABRICKS_HOST=&amp;lt;DATABRICKS HOST&amp;gt; export DATABRICKS_TOKEN=&amp;lt;DATABRICKS TOKEN&amp;gt; # Account needs permission to create clusters/dlt pipelines. Commit your local changes to your remote branch used above
Run integration test against cloudfile or eventhub or kafka using below options: 5a.</description></item></channel></rss>