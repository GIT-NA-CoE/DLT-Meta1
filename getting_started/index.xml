<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Getting Started on DLT-META</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/</link><description>Recent content in Getting Started on DLT-META</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 04 Aug 2021 14:50:11 -0400</lastBuildDate><atom:link href="https://databrickslabs.github.io/dlt-meta/getting_started/index.xml" rel="self" type="application/rss+xml"/><item><title>Metadata Preparation</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/metadatapreperation/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/metadatapreperation/</guid><description>Create onboarding.json metadata file and save to s3/adls/dbfs e.g.onboarding file
Create silver_transformations.json and save to s3/adls/dbfs e.g Silver transformation file
Create data quality rules json and store to s3/adls/dbfs e.g Data Quality Rules
&amp;quot;source_schema_path&amp;quot;: &amp;quot;tests/resources/schema/eventhub_iot_schema.ddl&amp;quot;, &amp;quot;eventhub.accessKeyName&amp;quot;: &amp;quot;iotIngestionAccessKey&amp;quot;, &amp;quot;eventhub.name&amp;quot;: &amp;quot;iot&amp;quot;, &amp;quot;eventhub.secretsScopeName&amp;quot;: &amp;quot;eventhubs_creds&amp;quot;, &amp;quot;kafka.sasl.mechanism&amp;quot;: &amp;quot;PLAIN&amp;quot;, &amp;quot;kafka.security.protocol&amp;quot;: &amp;quot;SASL_SSL&amp;quot;, &amp;quot;eventhub.namespace&amp;quot;: &amp;quot;ganesh-standard&amp;quot;, &amp;quot;eventhub.port&amp;quot;: &amp;quot;9093&amp;quot; Onboarding File structure env is your environment placeholder e.g dev, prod, stag</description></item><item><title>Build Python Wheel</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/buildwhl/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/buildwhl/</guid><description>This step is optional if you use PyPI dlt-meta
Clone DLT-META repo locally Launch terminal Goto root dlt-meta folder use command: python setup.py bdist_wheel Upload created python wheel file from dist folder to your blob storage like s3/adls/dbfs</description></item><item><title>Launch Onboarding job</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/launchonboardingjob/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/launchonboardingjob/</guid><description>Go to your Databricks landing page and do one of the following:
In the sidebar, click Jobs Icon Workflows and click Create Job Button.
In the sidebar, click New Icon New and select Job from the menu.
In the task dialog box that appears on the Tasks tab, replace Add a name for your jobâ€¦ with your job name, for example, Python wheel example.</description></item><item><title>Launch Generic DLT pipeline</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/dltpipeline/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/dltpipeline/</guid><description>1. Create a Delta Live Tables launch notebook Go to your Databricks landing page and select Create a notebook, or click New Icon New in the sidebar and select Notebook. The Create Notebook dialog appears.
In the Create Notebook dialogue, give your notebook a name e.g dlt_meta_pipeline and select Python from the Default Language dropdown menu. You can leave Cluster set to the default value. The Delta Live Tables runtime creates a cluster before it runs your pipeline.</description></item><item><title>Additionals</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/additionals/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/additionals/</guid><description>Run Integration Tests Launch Terminal/Command promt
Goto to DLT-META directory
Create environment variables.
export DATABRICKS_HOST=&amp;lt;DATABRICKS HOST&amp;gt; export DATABRICKS_TOKEN=&amp;lt;DATABRICKS TOKEN&amp;gt; # Account needs permission to create clusters/dlt pipelines. Commit your local changes to your remote branch used above
Run integration test against cloudfile or eventhub or kafka using below options: 5a. Run the command for cloudfiles python integration-tests/run-integration-test.py --cloud_provider_name=aws --dbr_version=11.3.x-scala2.12 --source=cloudfiles --dbfs_path=dbfs:/tmp/DLT-META/</description></item></channel></rss>